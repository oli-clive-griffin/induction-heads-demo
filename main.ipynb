{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import einops\n",
    "import random\n",
    "import wandb\n",
    "import PIL.Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = \"mps\"\n",
    "\n",
    "cache = {}\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, d_model, d_head, index) -> None:\n",
    "        super().__init__()\n",
    "        self.index = index\n",
    "        self.d_head = d_head\n",
    "        self.W_Q = nn.Linear(d_model, d_head)\n",
    "        self.W_K = nn.Linear(d_model, d_head)\n",
    "        self.W_V = nn.Linear(d_model, d_head)\n",
    "        self.W_O = nn.Linear(d_head, d_model)\n",
    "        self.attn_pattern = None\n",
    "    \n",
    "    def forward(self, x: Tensor, with_cache):\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        # x = F.scaled_dot_product_attention(\n",
    "        #     query=Q,\n",
    "        #     key=K,\n",
    "        #     value=V,\n",
    "        #     is_causal=True,\n",
    "        # )\n",
    "\n",
    "        Q = self.W_Q.forward(x) # (batch_size, seq_len, d_head)\n",
    "        K = self.W_K.forward(x) # (batch_size, seq_len, d_head)\n",
    "        V = self.W_V.forward(x) # (batch_size, seq_len, d_head)\n",
    "\n",
    "        QK_ = (Q @ K.transpose(-2, -1))\n",
    "        QK = QK_ / (QK_.shape[-1] ** 0.5)\n",
    "        large_negative_number = -1e9\n",
    "        mask = torch.ones_like(QK) * large_negative_number\n",
    "        mask = torch.triu(mask, diagonal=1)\n",
    "        attn = F.softmax(QK + mask, dim=-1)\n",
    "        self.attn_pattern = attn\n",
    "        if with_cache:\n",
    "            cache[f'Q_{self.index}'] = Q\n",
    "            cache[f'K_{self.index}'] = K\n",
    "            cache[f'V_{self.index}'] = V\n",
    "            cache[f'attn_{self.index}'] = attn\n",
    "        x = attn @ V\n",
    "        return self.W_O(x)\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, d_model, d_vocab, d_head) -> None:\n",
    "        super().__init__()\n",
    "        self.d_vocab = d_vocab\n",
    "        self.d_model = d_model\n",
    "        self.embed = nn.Embedding(d_vocab, d_model)\n",
    "        self.attn1 = AttentionLayer(d_model, d_head, 1)\n",
    "        self.attn2 = AttentionLayer(d_model, d_head, 2)\n",
    "        self.unembed = nn.Linear(d_model, d_vocab)\n",
    "\n",
    "    def forward(self, input: Tensor, with_cache):\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len)\n",
    "        \n",
    "        returns: logits (batch_size, seq_len, d_vocab)\n",
    "        \"\"\"\n",
    "        pos = self.positional_encoding(input.shape[1])\n",
    "        res = self.embed(input) + einops.rearrange(pos, 's d -> () s d')\n",
    "        res = res + self.attn1(res, with_cache)\n",
    "        res = res + self.attn2(res, with_cache)\n",
    "        logits = self.unembed(res)\n",
    "        return logits\n",
    "\n",
    "    def forward_train(self, x: Tensor, mask: Tensor):\n",
    "        \"\"\"\n",
    "        x:    (b, seq)\n",
    "        mask: (b, seq)\n",
    "        \"\"\"\n",
    "        input = x[:,:-1]\n",
    "        target = x[:,1:] # target = x[:,:-1] # BUG\n",
    "        mask = mask[:,1:] # TODO: check\n",
    "\n",
    "        pred_logits = self.forward(input, False)\n",
    "        pred_log_probs = F.log_softmax(pred_logits, dim=-1)\n",
    "        target_one_hot = F.one_hot(target, num_classes=self.d_vocab).float()\n",
    "        loss = (\n",
    "            einops.reduce(-target_one_hot * pred_log_probs, 'b s v -> b s', 'sum')\n",
    "            # * (~(mask.bool()))\n",
    "        ).mean()\n",
    "        return loss\n",
    "\n",
    "    def positional_encoding(self, seq_len):\n",
    "        \"\"\"\n",
    "        returns: (seq_len, d_model)\n",
    "        \"\"\"\n",
    "        pos = torch.arange(seq_len).float().unsqueeze(-1)\n",
    "        i = torch.arange(self.d_model // 2).float()\n",
    "        denom = 1 / (10000 ** (2 * i / self.d_model))\n",
    "        pe = torch.zeros(seq_len, self.d_model)\n",
    "        pe[:, 0::2] = torch.sin(pos * denom)\n",
    "        pe[:, 1::2] = torch.cos(pos * denom)\n",
    "        return pe.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Trainer:\n",
    "    def __init__(self, batch_size, seq_len_limit, model: Model):\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len_limit = seq_len_limit\n",
    "        self.model = model\n",
    "        self.step = 0\n",
    "\n",
    "    def train(self, epochs):\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=1e-3)\n",
    "        for e in range(epochs):\n",
    "            optimizer.zero_grad()\n",
    "            x, mask = self.create_example()\n",
    "            loss = self.model.forward_train(x, mask)\n",
    "            loss.backward()\n",
    "            print(f\"Epoch {e}, loss: {loss.item()}\")\n",
    "            optimizer.step()\n",
    "            attn_pattern_1 = self.model.attn1.attn_pattern[0].detach().cpu().numpy()\n",
    "            attn_pattern_1_as_img = PIL.Image.fromarray(wandb.Image.to_uint8(attn_pattern_1))\n",
    "            attn_pattern_2 = self.model.attn2.attn_pattern[0].detach().cpu().numpy()\n",
    "            attn_pattern_2_as_img = PIL.Image.fromarray(wandb.Image.to_uint8(attn_pattern_2))\n",
    "            wandb.log({\"loss\": loss}, self.step)\n",
    "            if e % 50 == 0:\n",
    "                wandb.log({\"attn_pattern_1\": wandb.Image(attn_pattern_1_as_img), \"attn_pattern_2\": wandb.Image(attn_pattern_2_as_img)})\n",
    "            self.step += 1\n",
    "\n",
    "    def create_example(self):\n",
    "        x = []\n",
    "        mask = []\n",
    "        for _ in range(self.batch_size):\n",
    "            s_chunk_len = random.randint(self.seq_len_limit // 4, self.seq_len_limit // 2) # // 2 so that we have at least 2 whole chunks\n",
    "            seq = []\n",
    "            l = list(range(self.model.d_vocab))\n",
    "            random.shuffle(l)\n",
    "            chunk = l[:s_chunk_len]\n",
    "            while len(seq) < self.seq_len_limit:\n",
    "                seq += chunk\n",
    "            seq = seq[:self.seq_len_limit]\n",
    "            x.append(seq)\n",
    "            mask.append([1] * s_chunk_len + [0] * (self.seq_len_limit - s_chunk_len))\n",
    "\n",
    "        return torch.tensor(x).to(device), torch.tensor(mask).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_vocab = 1000\n",
    "d_model = 512\n",
    "d_head = 64\n",
    "model = Model(d_model=d_model, d_vocab=d_vocab, d_head=d_head).to(device)\n",
    "wandb.init(project=\"trying-to-find-induction-heads\")\n",
    "trainer = Trainer(batch_size=256, seq_len_limit=128, model=model)\n",
    "trainer.train(1000)\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_enc = model.positional_encoding(seq_len=127)\n",
    "\n",
    "QK = model.attn1.W_Q.weight.T @ model.attn1.W_K.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def try_pair(a, b):\n",
    "    x = einops.einsum(QK, pos_enc[b], ' m n, n -> m')\n",
    "    return einops.einsum(x, pos_enc[a], 'm, m ->').detach().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "scores = [try_pair(a, a+1) for a in range(126)]\n",
    "plt.plot(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from matplotlib.pyplot import imshow\n",
    "a = (pos_enc @ QK @ pos_enc.T)\n",
    "mask = torch.ones_like(a)\n",
    "mask = torch.tril(mask, diagonal=0)\n",
    "x = a * mask\n",
    "imshow(x.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "out = model.forward(\n",
    "    torch.tensor([[100, 200, 300, 400, 500, 600, 500, 500, 500]]).to(device),\n",
    "    True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from matplotlib.pyplot import imshow\n",
    "torch.set_printoptions(precision=4, sci_mode=False)\n",
    "\n",
    "QK = model.attn1.W_Q.weight.T @ model.attn1.W_K.weight\n",
    "print(QK)\n",
    "imshow(QK.detach().cpu().numpy())\n",
    "\n",
    "OV_l1 = model.attn1.W_O.weight @ model.attn1.W_V.weight\n",
    "OV_l2 = model.attn2.W_O.weight @ model.attn2.W_V.weight\n",
    "\n",
    "OV = OV_l2 @ OV_l1\n",
    "\n",
    "OV.std(), OV.mean(), OV.median(), OV.min(), OV.max()\n",
    "\n",
    "QK = model.attn2.W_Q.weight.T @ model.attn2.W_K.weight\n",
    "\n",
    "imshow(QK.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "probs = F.softmax(out, dim=-1).argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.attn1.W_O.weight = saved1\n",
    "model.attn2.W_O.weight = saved2\n",
    "#%%\n",
    "\n",
    "model.attn1.W_O.weight = nn.Parameter(torch.zeros_like(model.attn1.W_O.weight))\n",
    "model.attn2.W_O.weight = nn.Parameter(torch.zeros_like(model.attn2.W_O.weight))\n",
    "\n",
    "model.attn1.W_O.weight = nn.Parameter(torch.zeros_like(model.attn1.W_O.weight))\n",
    "model.attn2.W_O.weight = nn.Parameter(torch.zeros_like(model.attn2.W_O.weight))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
